usefull info on maskrcnn

### grab more info / more reading
#to optimize GPU memory: https://github.com/matterport/Mask_RCNN/wiki
#https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46


### info on how its working
*The network generates the bounding boxes first, and then filters them to pick the most accurate ones, and then finally applies the mask branch only on the detected boxes. This arrangement actually makes the network faster because it allows it to run the mask branch (which is a heavy branch) only on the boxes that have already been filtered.
*the classification comes before generating the pixel by pixel. 
*object-detection and mask are parallel run
*Masks are generated at the pixel level. In other words, each pixels decides for itself if it's part of the mask or not. So the mask can take any shape, if you have binary masks (then simply use them directly Skiping the binary->polygon->binary conversion which treats the masks of holes as the object mask during training). If you want to have hole in your masks and dontahve bianry mask, then you can create one mask with a bridge between holes for example. https://github.com/matterport/Mask_RCNN/issues/933
*A test you can do to rule out any technical issues is to test on the training data. If masks look good on the training data but bad on the validation data, then this confirms that you need a bigger dataset.
*Further, the RPN doesn’t scan over the image directly. Instead, the RPN scans over the backbone feature map. 
*If several anchors overlap too much, we keep the one with the highest foreground score and discard the rest (referred to as Non-max Suppression). After that we have the final proposals (regions of interest) that we pass to the next stage.
*ROI pooling refers to cropping a part of a feature map and resizing it to a fixed size. It’s similar in principle to cropping part of an image and then resizing it (but there are differences in implementation details).
*Faster RCNN: same as maskrcc without segmentation
*Mask_RCNN networks implementation is not meant for BG alone too. Because they have internally ROI(Region of Interest) layer.
*overlapping bounding boxes/masks can be located on the same object with different class. These overlapping masks are not filtered by NMS because they're from different classes, the network is predicting the same object, with almost exactly the same mask, but assigning different labels to those predictions. (https://github.com/matterport/Mask_RCNN/issues/809)

### maskrcnn output 
results = model.detect([image1,image2], verbose=0)
r1 = results[0] #take results of the image1
r2 = results[1] #take results of the image2
li_bboxes = [(b[1], b[0], b[3]-b[1], b[2]-b[0]) for b in r1['rois']] #x,y,w,h while maskrcnn output x1,y1,x2,y2
li_bmasks = r1['masks'] #binary masks
print('There is %d masks'%len(li_bmasks[0][0])) #first mask: li_bmasks[:,:,0], second mask: li_bmasks[:,:,1]

r1 example:
{'class_ids': array([5]), 'masks': array([[[False],
         [False],
         [False],
         ...,
         [False],
         [False],
         [False]],
 
        ...,
 
        [[False],
         [False],
         [False],
         ...,
         [False],
         [False],
         [False]]]), 'rois': array([[158, 258, 239, 416]]), 'scores': array([0.35618505], dtype=float32)}

### code
fucntion that 'gives the images': in model.py of mask-rcnn code check the load_image_gt() function, which use: image = dataset.load_image(image_id), same as in inspect_model notebook


## training
1. as expected: must train again if change the numebr of class: https://github.com/matterport/Mask_RCNN/issues/1513
2. ROI mini batch set to 512 is to high when you have small amount of obect in the picture, as it wont lead to 1:3 positive to negative ratio (ratio suggested by the paper). Hence reduce to 128 for example when you want not to overfit the negative ROIs https://github.com/matterport/Mask_RCNN/issues/43

3.:
def train(model):
    
    path_ = path_image
    
    """Train the model."""
    #training dataset.
    dataset_train = utils_data_class_and_config.VGG_Dataset()
    dataset_train.load_vgg(path_, "train", remove_MA=False)
    dataset_train.prepare()

    #validation dataset
    dataset_val = utils_data_class_and_config.VGG_Dataset()
    dataset_val.load_vgg(path_, "val", remove_MA=False)
    dataset_val.prepare()

    #training stage 1
    #print("Training network heads")
    #model.train(dataset_train, dataset_val,
    #            learning_rate = config.LEARNING_RATE,
    #            epochs = 50,
    #            layers = 'heads')  #augmentation=aug
    
    #training - Stage 2
    # Finetune layers from ResNet stage 4 and up
    #print("Fine tune Resnet stage 4 and up")
    #model.train(dataset_train, dataset_val,
    #            learning_rate = config.LEARNING_RATE/10,
    #            epochs = 90,
    #            layers = '3+', augmentation=aug) #3+/4+

    #training - Stage 3
    # Fine tune all layers
    #print("Fine tune all layers")
    model.train(dataset_train, dataset_val,
                learning_rate=config.LEARNING_RATE/10,
                epochs=75,
                layers='all')
    
    model.train(dataset_train, dataset_val,
                learning_rate=config.LEARNING_RATE/100,
                epochs=90,
                layers='all')



###basics to copy paste
####load model with correct batchsize ####
#Override the training configurations with a few changes for inferencing.
class InferenceConfig(config.__class__):
    # Run detection on one image at a time
    GPU_COUNT = 1
    IMAGES_PER_GPU = batch_size
    #BATCH_SIZE = GPU_COUNT * IMAGES_PER_GPU

config = InferenceConfig()
#Create model in inference mode
model = modellib.MaskRCNN(mode='inference', model_dir=MODEL_DIR, config=config)
#load weights
weights_path = os.path.join(ROOT_DIR, 'logs', id_, ep+'.h5') 
model.load_weights(weights_path, by_name=True)

####take correct number of images acccorading to batch_size ####
files = glob.glob(os.path.join(VGG_DIR, 'test', '*.png'))
d = len(files)%batch_size
files = files[0:(len(files)-d)]

####first prediciton takes longeur, so we wont use it in fps count ####
#keras needs to allocate memory and do other stuff before the first inference
#https://github.com/keras-team/keras/issues/8724
#https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/PDIBnp1ftxk
results = model.detect([cv2.imread(f) for f in files[0:batch_size]], verbose=0)    
